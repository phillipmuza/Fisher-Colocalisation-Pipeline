### Script file for data analysis of GFAP+S100B Colocalisation
  ### Updated so its more concise - 15.02.23
### Author: Phillip Muza
### Date: 26.01.23

###Change according to your experimental needs

#### Load packages and functions #### 
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, crayon)

#### 1. Read your merged dataframes and volume table and combine them together ####
read_dataframes <- function(){
  cells_df = list.files(pattern = "*merged.csv", ignore.case = TRUE)
  volume_df = list.files(pattern = "AllPixels.txt", ignore.case = TRUE)
  files_needed <- list(cells_df, volume_df) #this are the files required to run this function
  
  #this for statement catches any missing files within your WD and stops the function if files are missing
  for (files in files_needed){
    if(!file.exists(files)){
      stop(paste(files, ': Merged file missing. Please re-run:', getwd())) 
    }
  }
  
  df <- read.csv(cells_df) #reads merged_df.csv
  
  #sometimes there are errors in how AllPixels is encoded - tryCatch will catch any errors and report to you
  vol <- tryCatch({read.table(volume_df, header = F)}, #reads AllPixels.txt
                  error = function(e){
                    cat(crayon::bgRed('An error occurred:', conditionMessage(e), '\n'))
                    return(NULL)
  })
  
  if (is.null(vol)){
    cat(crayon::bgRed('Function stopped due to an error. Check:', getwd(), 'for errors in AllPixels.txt.\n'))
    return(NULL)
  }
  
  colnames(vol) <- c("x", "y", "z", "pixel_values")
  voxel_size <- length(vol$x) * (2^3) #voxel size is X^3 where X=z-step size (or downsampling factor)
  volume <- voxel_size / (1000^3) #volume = mm^3 
  df$region_volume.mm3. <- volume #Add volume to the decode_dataframe
  return(df)
}

#### 2.This for loop will go through each parent directory and sub-directory and generate a list of dataframes ####
read_dataframes_loop <- function(){
  dataframes = list() #Initialise an empty list to store your dataframes
  for (parent in parent_directories){ #Loop through all parent directories
    setwd(parent)
    decoded <- subset(animal_decoded, blinded_number == basename(getwd())) #decoded table for the given folder/animal
    sub_directories = list.dirs(getwd(), full.names = TRUE) #create a variable for subfolders in the given parent directory
    
    #Loop through your sub-directories
    for (sub in sub_directories){
      setwd(sub)
      cat(crayon::blue('Now working in:', sub, '\n'))
      
    #This if statement skips through all folders without the necessary files to run the function
      if(file.exists('AllPixels.txt')){
        df <- read_dataframes() #read your cell table and volume tables
      } else {
        cat(crayon::bgRed('Working directory does not have required files. Moving on to next directory.\n'))
        next
      }
      
      #This tryCatch will catch any errors generated by the read_dataframes() function
      tryCatch({
        df2 <- cbind(df, decoded) #combine your cell and volume dataframes with your decoded dataframe for the given folder/animal
        dataframes[[sub]] <- df2 #append df2 into your dataframes list
      }, error = function(e){
        cat('Error occured with binding dataframes:', conditionMessage(e), '\n')
      })
    next}
  }
  
  return(dataframes)
}

#Tidy your dataframe using this function
clean_dataframe <- function(){
  combined_data <- bind_rows(list_of_dataframes) #combine list of dataframes into one dataframe
  cleaned_dataframe <- subset(combined_data, select = -c(X, x.coord, y.coord, z.coord, euclidean_distance)) #Remove columns not required
  cleaned_dataframe[is.na(cleaned_dataframe)] <-"not_applicable"
  return(cleaned_dataframe)
}

#Summarise and group data 
summarise_dataframe <- function(){
  summary_df <- clean_df %>%
    group_by(blinded_number, genotype, brain_region, sex, Label, colocalisation) %>%
    summarise(total_objects = n(),
              mean_region_volume.mm3. = mean(region_volume.mm3.),
              mean_cell_volume = mean(Volume.um3.),
              total_cell_volume.mm3. = sum(Volume.um3.) / (1000^3)
    )
  summary_df$cell_density.cells.mm3. <- summary_df$total_objects * summary_df$mean_region_volume.mm3.
  summary_df$cell_coverage.100percent. <- (summary_df$total_cell_volume.mm3. / summary_df$mean_region_volume.mm3.) * 100
  return(summary_df)
}

#### Run your script from here ####

#Set Working Directory
setwd("C://Users//phill//OneDrive//Documents//colocalisations_glia//coloc_analysis//CA1")

#Upload your table with your decoded data
animal_decoded <- read.csv("../blinded_df.csv")


#Set up a variable for your parent folders
  #We will loop through these to decode and set up our big dataframe
parent_directories = list.dirs(getwd(), full.names = TRUE, recursive = FALSE)

#3.Generate your dataframes
list_of_dataframes <- read_dataframes_loop()

#4.Tidy your list of dataframes
clean_df <- clean_dataframe()

#You can add a column describing your brain region - replace "CA1" with the description
clean_df$brain_region <- "CA1"

#5.Summarise and group your data
summarised_df <- summarise_dataframe()

#6. Build your dataset 
  ## NOT RUN ##
CA1_clean <- summarise_dataframe()
#CA3_clean <- summarise_dataframe()
#GCL_clean <- summarise_dataframe()
#ML_PML_clean <- summarise_dataframe()

summarised_dataset <- rbind(CA1_clean, CA3_clean, GCL_clean, ML_PML_clean)

#CA1_long <- clean_df
#CA3_long <- clean_df
#GCL_long <- clean_df
#ML_PML_long <- clean_df

raw_dataset <- rbind(CA1_long, CA3_long, GCL_long, ML_PML_long)
  